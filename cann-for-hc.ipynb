{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8166679,"sourceType":"datasetVersion","datasetId":4832606},{"sourceId":8182985,"sourceType":"datasetVersion","datasetId":4815699}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"CODE_SIZE = 64 * 64\nVAL_TRUTH_PATH = \"/kaggle/input/hypertension-classification/validation_truth.txt\"\nVAL_SET_PATH = \"/kaggle/input/hypertension-classification/2. Validation Set\"\nTRAIN_SET_PATH = \"/kaggle/input/hypertension-classification/AE Training set\"\nFPRATE = 0.2\nEPOCHS = 30\n\nimport torch\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:37:26.951816Z","iopub.execute_input":"2024-04-22T03:37:26.952188Z","iopub.status.idle":"2024-04-22T03:37:26.958192Z","shell.execute_reply.started":"2024-04-22T03:37:26.952158Z","shell.execute_reply":"2024-04-22T03:37:26.957326Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\n\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        \n        self.encoder1 = nn.Conv2d(3, 128, kernel_size=5, stride=1, padding=2)\n        self.encoder2 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n        self.encoder3 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n        self.encoder4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.encoder5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.encoder6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.encoder7 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=4, stride=4)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=4)\n        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.decoder1 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=2)\n        self.decoder2 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n        self.decoder3 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n        self.decoder4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.decoder5 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.decoder6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.decoder7 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        \n    def encode1(self, x):\n        x = F.leaky_relu(self.encoder1(x))\n        x = F.leaky_relu(self.encoder2(x))\n        x = self.maxpool1(x)\n        return x\n        \n    def decode1(self, x):\n        x = self.upsample1(x)\n        x = F.sigmoid(self.decoder2(x))\n        x = F.sigmoid(self.decoder1(x))\n        return x\n        \n    def encode2(self, x):\n        x = F.leaky_relu(self.encoder3(x))\n        x = F.leaky_relu(self.encoder4(x))\n        x = self.maxpool2(x)\n        return x\n        \n    def decode2(self, x):\n        x = self.upsample2(x)\n        x = F.leaky_relu(self.decoder4(x))\n        x = F.leaky_relu(self.decoder3(x))\n        return x\n    \n    def encode3(self, x):\n        x = F.leaky_relu(self.encoder5(x))\n        x = F.leaky_relu(self.encoder6(x))\n        x = self.maxpool1(x)\n        x = F.leaky_relu(self.encoder7(x))\n        return x\n        \n    def decode3(self, x):\n        x = F.leaky_relu(self.decoder7(x))\n        x = self.upsample1(x)\n        x = F.leaky_relu(self.decoder6(x))\n        x = F.leaky_relu(self.decoder5(x))\n        return x\n        \n    def encode(self, x):\n        x = self.encode1(x)\n        x = self.encode2(x)\n        x = self.encode3(x)\n        return x\n    \n    def decode(self, x):\n        x = self.decode3(x)\n        x = self.decode2(x)\n        x = self.decode1(x)\n        return x\n\n    def forward(self, x):\n        x = self.encode(x)\n        x = self.decode(x)\n        return x\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T03:34:07.404819Z","iopub.execute_input":"2024-04-22T03:34:07.405390Z","iopub.status.idle":"2024-04-22T03:34:07.427851Z","shell.execute_reply.started":"2024-04-22T03:34:07.405356Z","shell.execute_reply":"2024-04-22T03:34:07.426951Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass ImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_files = os.listdir(root_dir)\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.image_files[idx])\n        image = Image.open(img_name).resize((128, 128))\n        if self.transform:\n            image = self.transform(image)\n        return image","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:34:07.429651Z","iopub.execute_input":"2024-04-22T03:34:07.430039Z","iopub.status.idle":"2024-04-22T03:34:07.438783Z","shell.execute_reply.started":"2024-04-22T03:34:07.430005Z","shell.execute_reply":"2024-04-22T03:34:07.437779Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n\n\"\"\"def train(load = False):\n    transform =  transforms.Compose([\n        transforms.Resize((128,128)),\n        transforms.ToTensor()\n    ])\n    dataset = ImageDataset(TRAIN_SET_PATH, transform)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    autoencoder = Autoencoder().to(device)\n    if load:\n        state_dict = torch.load(\"/kaggle/working/autoencoder1.pth\")\n        autoencoder.load_state_dict(state_dict)\n\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.0001)\n    try:\n        for epoch in range(1, EPOCHS+1):\n            running_loss = 0.0\n            for images in tqdm(dataloader):\n                images = images.to(device)\n                optimizer.zero_grad()\n                mids = autoencoder.encode1(images)\n                outputs = autoencoder.decode1(mids)\n                loss = criterion(outputs, images)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item() * images.size(0)\n            epoch_loss = running_loss / len(dataset)\n            print(f\"Epoch [{epoch}/{EPOCHS}], Loss: {epoch_loss:.8f}\")\n        for epoch in range(1, EPOCHS+1):\n            running_loss = 0.0\n            for images in tqdm(dataloader):\n                images = images.to(device)\n                optimizer.zero_grad()\n                mids = autoencoder.encode1(images)\n                outputs = autoencoder.encode2(mids)\n                outputs = autoencoder.decode2(outputs)\n                loss = criterion(outputs, mids)\n                outputs = autoencoder.decode1(outputs)\n                loss.backward()\n                loss2 = criterion(outputs, images)\n                optimizer.step()\n                running_loss += loss2.item() * images.size(0)\n            epoch_loss = running_loss / len(dataset)\n            print(f\"Epoch [{epoch}/{EPOCHS}], Loss: {epoch_loss:.8f}\")\n        for epoch in range(1, EPOCHS+1):\n            running_loss = 0.0\n            for images in tqdm(dataloader):\n                images = images.to(device)\n                optimizer.zero_grad()\n                mids = autoencoder.encode1(images)\n                mids = autoencoder.encode2(mids)\n                outputs = autoencoder.encode3(mids)\n                outputs = autoencoder.decode3(outputs)\n                loss = criterion(outputs, mids)\n                loss.backward()\n                outputs = autoencoder.decode2(outputs)\n                outputs = autoencoder.decode1(outputs)\n                loss2 = criterion(outputs, images)\n                optimizer.step()\n                running_loss += loss2.item() * images.size(0)\n            epoch_loss = running_loss / len(dataset)\n            print(f\"Epoch [{epoch}/{EPOCHS}], Loss: {epoch_loss:.8f}\")\n        for epoch in range(1, EPOCHS+1):\n            running_loss = 0.0\n            for images in tqdm(dataloader):\n                images = images.to(device)\n                optimizer.zero_grad()\n                outputs = autoencoder(images)\n                loss = criterion(outputs, images)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item() * images.size(0)\n            epoch_loss = running_loss / len(dataset)\n            print(f\"Epoch [{epoch}/{EPOCHS}], Loss: {epoch_loss:.8f}\")\n    finally:\n        torch.save(autoencoder.state_dict(), \"autoencoder1.pth\")\n    return autoencoder\n\n\"\"\"\ndef train(load = False):\n    transform =  transforms.Compose([\n        transforms.Resize((128,128)),\n        transforms.ToTensor()\n    ])\n    dataset = ImageDataset(TRAIN_SET_PATH, transform)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    autoencoder = Autoencoder().to(device)\n    if load:\n        state_dict = torch.load(\"/kaggle/working/autoencoder.pth\")\n        autoencoder.load_state_dict(state_dict)\n        \n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.0001)\n    try:\n        for epoch in range(1, EPOCHS+1):\n            running_loss = 0.0\n            for images in tqdm(dataloader):\n                images = images.to(device)\n                optimizer.zero_grad()\n                outputs = autoencoder(images)\n                loss = criterion(outputs, images)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item() * images.size(0)\n            epoch_loss = running_loss / len(dataset)\n            print(f\"Epoch [{epoch}/{EPOCHS}], Loss: {epoch_loss:.8f}\")\n    finally:\n        torch.save(autoencoder.state_dict(), \"autoencoder.pth\")\n    return autoencoder","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:34:07.440130Z","iopub.execute_input":"2024-04-22T03:34:07.440405Z","iopub.status.idle":"2024-04-22T03:34:07.454629Z","shell.execute_reply.started":"2024-04-22T03:34:07.440381Z","shell.execute_reply":"2024-04-22T03:34:07.453791Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def getThreshold(FPrate = 0.1):\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor()\n    ])\n    dataset = ImageDataset(TRAIN_SET_PATH, transform)\n    dataloader = DataLoader(dataset, batch_size=1)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    autoencoder = Autoencoder().to(device)\n    state_dict = torch.load(\"/kaggle/working/autoencoder1.pth\")\n    autoencoder.load_state_dict(state_dict)\n    criterion = torch.nn.MSELoss()\n    losses = []\n    for images in dataloader:\n        images = images.to(device)\n        outputs = autoencoder(images)\n        loss = criterion(outputs, images)\n        losses.append(loss.item())\n    losses = sorted(losses)\n    print(-int(len(losses)*FPrate))\n    return losses[-int(len(losses)*FPrate)]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:34:07.456903Z","iopub.execute_input":"2024-04-22T03:34:07.457369Z","iopub.status.idle":"2024-04-22T03:34:07.465335Z","shell.execute_reply.started":"2024-04-22T03:34:07.457336Z","shell.execute_reply":"2024-04-22T03:34:07.464419Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def validate(threshold=0.004):\n    transform =  transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor()\n    ])\n    dataset = ImageDataset(VAL_SET_PATH, transform)\n    dataloader = DataLoader(dataset, batch_size=1)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    autoencoder = Autoencoder().to(device)\n    state_dict = torch.load(\"/kaggle/working/autoencoder1.pth\")\n    autoencoder.load_state_dict(state_dict)\n    criterion = torch.nn.MSELoss()\n    abnormal = np.array([])\n    for images in dataloader:\n        images = images.to(device)\n        outputs = autoencoder(images)\n        loss = criterion(outputs, images)\n        if loss.item() > threshold:\n            abnormal = np.append(abnormal, 1)\n        else:\n            abnormal = np.append(abnormal, 0)\n    return abnormal\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:34:07.466443Z","iopub.execute_input":"2024-04-22T03:34:07.466775Z","iopub.status.idle":"2024-04-22T03:34:07.476836Z","shell.execute_reply.started":"2024-04-22T03:34:07.466742Z","shell.execute_reply":"2024-04-22T03:34:07.475946Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"autoencoder = train(False)\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor()\n])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nidx = 44\n# print(thres)\n# ae = Autoencoder().to(device)\n# state_dict = torch.load(\"/kaggle/input/model-for-hc/autoencoder_part1.pth\")\n# ae.load_state_dict(state_dict)\ndataset = ImageDataset(TRAIN_SET_PATH, transform)\ntransforms.ToPILImage()(dataset[idx]).save(f\"origin{idx}.png\")\nimg = dataset[idx].to(\"cuda\")\nimages = img.view(1, 3, 128, 128)\noutputs = autoencoder(images)\nimg = transforms.ToPILImage()(outputs[0])\nimg.save(f\"test{idx}.png\")\n\nwith open(VAL_TRUTH_PATH, 'r') as f:\n    valtruths = f.readlines()\nvaltruths = np.array([int(valtruth[:-1]) for valtruth in valtruths])\nthres = getThreshold(FPRATE)\nprint(thres)\nabnormals = validate(thres)\nFP = np.where(np.logical_and(abnormals==1,valtruths<=0),1,0)\nTP = np.where(np.logical_and(abnormals==1,valtruths>=1),1,0)\nFN = np.where(np.logical_and(abnormals==0,valtruths>=1),1,0)\nTN = np.where(np.logical_and(abnormals==0,valtruths<=0),1,0)\nPrecision = np.sum(TP)/(np.sum(TP)+np.sum(FP))\nRecall = np.sum(TP)/(np.sum(TP)+np.sum(FN))\nF1_score = 2*Precision*Recall/(Precision+Recall)\nFP_rate = np.sum(FP)/(np.sum(FP)+np.sum(TN))\nprint(np.sum(FP),np.sum(TP),np.sum(FN),np.sum(TN))\nprint(\"Precision:\",Precision,\"Recall:\",Recall, \"F1 Score:\", F1_score, \"FP Rate:\", FP_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:37:33.547075Z","iopub.execute_input":"2024-04-22T03:37:33.547441Z","iopub.status.idle":"2024-04-22T03:45:30.028542Z","shell.execute_reply.started":"2024-04-22T03:37:33.547413Z","shell.execute_reply":"2024-04-22T03:45:30.027672Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/30], Loss: 0.06051075\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/30], Loss: 0.03126010\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/30], Loss: 0.02080905\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/30], Loss: 0.01497395\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/30], Loss: 0.01226766\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/30], Loss: 0.01038405\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/30], Loss: 0.00897426\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/30], Loss: 0.00786391\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/30], Loss: 0.00703798\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/30], Loss: 0.00609869\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/30], Loss: 0.00558028\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/30], Loss: 0.00508429\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/30], Loss: 0.00469022\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/30], Loss: 0.00413410\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/30], Loss: 0.00386176\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/30], Loss: 0.00357788\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/30], Loss: 0.00373288\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/30], Loss: 0.00341159\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/30], Loss: 0.00326019\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/30], Loss: 0.00314129\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [21/30], Loss: 0.00310964\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [22/30], Loss: 0.00312486\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [23/30], Loss: 0.00317623\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [24/30], Loss: 0.00293670\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [25/30], Loss: 0.00276894\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [26/30], Loss: 0.00268470\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [27/30], Loss: 0.00268967\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [28/30], Loss: 0.00263352\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [29/30], Loss: 0.00252392\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [30/30], Loss: 0.00247219\n-80\n0.01448456384241581\n27 47 121 53\nPrecision: 0.6351351351351351 Recall: 0.27976190476190477 F1 Score: 0.38842975206611574 FP Rate: 0.3375\n","output_type":"stream"}]}]}